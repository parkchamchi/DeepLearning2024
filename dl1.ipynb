{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL1\n",
    "24.08.23\n",
    "\n",
    "## 학습 내용\n",
    "\n",
    "### Artificial Neural Networks\n",
    "- Perceptron을 여러개 배치한 모델로, 초기에 인간의 두뇌를 모방하여 만들어졌다.\n",
    "\n",
    "#### Multilayer Perceptrons (MLP)\n",
    "- Layer를 겹겹이 쌓은 것으로, 입력층 및 출력층 사이에 있는 layer들을 hidden layer라고 한다. 이 hidden layer의 정도를 depth라고 하는데, 여기서 \"deep learning\"이라는 용어가 나왔다.\n",
    "\n",
    "- Hidden layer에서는 대부분 affine transormation를 이용한다.\n",
    "$$ \\boldsymbol{y} = \\phi(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}) $$\n",
    "\n",
    "- Activation function (위 $\\phi$)는 sigmoid 함수나 ReLU 또는 그 변형이 사용된다.\n",
    "  - Logistic: $ y = \\frac{1}{1 + e^{-z}} $\n",
    "  - Tanh\n",
    "  - ReLU: $ \\max(0, z) $\n",
    "\n",
    "### Training Multilayer Perceptrons\n",
    "\n",
    "- 머신러닝에서 예측값($ \\hat{y} $)과 실제값($ y $)의 차이를 측정하기 위해 MSE, Cross-Entropy 등 Loss Function ($ J(\\boldsymbol{\\theta}) $)이 사용된다. 이때 보통 손실함수의 기울기를 따라 내려가 global minimum을 찾는 Gradient Descent 및 그 변형이 사용된다.\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) $$\n",
    "- Cross-Entropy: $ -\\frac{1}{m} \\sum^{m}_{i = 1} \\sum^{K}_{k = 1} y_k^{(i)} \\log(\\hat{p}_k^{(i)}) $\n",
    "\n",
    "MLP에서는 이 과정에서 Back-propagtion을 이용하는데, Forward-propagation에서 발생한 차이를 Chain rule과 Computational graphs를 활용해 역으로 나아가면서 weight값들을 수정한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Deep Learning\" by Goodfellow et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p61. Multivariate normal distribution\n",
    "\n",
    "Gaussian distribution\n",
    "$$ \\mathcal{N}(x; \\mu, \\sigma^2) = \\sqrt{\\frac{1}{2\\pi\\sigma^2}}\\exp(-\\frac{1}{2\\sigma^2}(x - \\mu)^2) $$\n",
    "또는 *precision parameter* $ \\beta = 1 / \\sigma^2 = 1 / \\mathrm{Variance}$ 을 이용하여\n",
    "$$ \\mathcal{N}(x; \\mu, \\beta^{-1}) = \\sqrt{\\frac{\\beta}{2\\pi}}\\exp(-\\frac{\\beta}{2}(x - \\mu)^2) $$\n",
    "\n",
    "이를 $ \\mathbb{R}^{n} $ 으로 generalize하면 ($ \\boldsymbol{\\Sigma} $ 은 \"covariance matrix of the distribution\" & is positive definitive symmetric matrix)\n",
    "$$ \\mathcal{N}(\\boldsymbol{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\sqrt{\\frac{1}{(2\\pi)^{n}\\det(\\boldsymbol{\\Sigma})}}\\exp(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu})^\\intercal \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu})) \\tag{p61.1}$$\n",
    "Known as **multivariate normal distribution**.\n",
    "\n",
    "또는 *precision matrix* $ \\boldsymbol{\\beta} = \\boldsymbol{\\Sigma}^{-1}$ 으로\n",
    "$$ \\mathcal{N}(\\boldsymbol{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\beta}^{-1}) = \\sqrt{\\frac{\\det(\\boldsymbol{\\beta})}{(2\\pi)^{n}}}\\exp(-\\frac{1}{2}({\\boldsymbol{x}} - \\boldsymbol{\\mu})^\\intercal \\boldsymbol{\\beta} (\\boldsymbol{x} - \\boldsymbol{\\mu})) $$\n",
    "\n",
    "#### Q.\n",
    "- 이때 이 $ \\mathbb{R}^{n} $ 상의 정규분포 식 `(p61.1)`은 어떻게 얻어지나요? (특히 루트 안)\n",
    "- 이 식에서 공분산 매트릭스 $ \\boldsymbol{\\Sigma} $ 는 어떤 역할을 하며, 특히 $ \\det(\\boldsymbol{\\Sigma}) $ 은 어떻게 스칼라 식의 $ \\sigma^2 $ 의 역할을 하나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### A.\n",
    "##### Cov. matrix\n",
    "$$ \\boldsymbol{\\Sigma} = \\mathrm{Cov}[\\bm{x}] = \\mathbb{E}[(x - \\mathbb{E}[\\bm{x}])(x - \\mathbb{E}[\\bm{x}])^{\\intercal}] $$\n",
    "$$ = \\mathbb{E}[\\bm{x}\\bm{x}^{\\intercal} - \\bm{\\mu}\\bm{x}^{\\intercal} - \\bm{x}\\bm{\\mu}^{\\intercal} + \\bm{\\mu}\\bm{\\mu}^{\\intercal}]  $$\n",
    "$$ = \\mathbb{E}[\\bm{x}\\bm{x}^{\\intercal}] - \\bm{\\mu}\\bm{\\mu}^{\\intercal} $$\n",
    "$$ (\\because \\mathbb{E}[\\bm{x}] = \\mathbb{E}[\\bm{x}^{\\intercal}] = \\bm{\\mu}) $$\n",
    "\n",
    "스칼라 식의 $ \\sigma^2 $ 가 값을 normalize해주는 것과 같이 벡터 식에서 $ \\det(\\boldsymbol{\\Sigma}) $ 가 $ \\bm{x} $ 의 *크기*에 따라 normalize한다. (어떻게?)\n",
    "\n",
    "#### Object\n",
    "Show $ \\int_{\\mathbb{R}^{n}} \\mathcal{N} (\\bm{x}; \\bm{\\mu}, \\bm{\\Sigma}) d\\bm{x} = \\int_{\\mathbb{R}^{n}} \\sqrt{\\frac{1}{(2\\pi)^{n}\\det(\\boldsymbol{\\Sigma})}}\\exp(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu})^\\intercal \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu})) d\\bm{x} = 1 $\n",
    "\n",
    "#### Proof\n",
    "Consider $ \\bm{y} =  \\bm{\\Sigma}^{-\\frac{1}{2}} (\\bm{x} - \\bm{\\mu})$, where $ \\bm{\\Sigma}^{-\\frac{1}{2}} \\bm{\\Sigma}^{-\\frac{1}{2}} = \\bm{\\Sigma}^{-1} $ (\"Then $ \\bm{y} $ follows a standard multivariate normal distribution w/ zero mean and identity cov. matrix\")\n",
    "\n",
    "Then\n",
    "$$ \\int_{\\mathbb{R}^{n}} \\mathcal{N} (\\bm{y}; \\bm{0}, \\bm{I}) d\\bm{y} = \\int_{\\mathbb{R}^{n}} \\sqrt{\\frac{1}{(2\\pi)^{n}}}\\exp(-\\frac{1}{2}\\bm{y}^\\intercal \\bm{y}) d\\bm{y} $$\n",
    "$$ (\\because \\det(\\bm{I}) = 1) $$\n",
    "Since each scalar is a normal distribution with $ \\mu_i = 0, \\sigma_i = 1 $, the integral is $ 1 $. `(p61.a.1)`\n",
    "\n",
    "Back on $ \\bm{x} $,\n",
    "$$ \\int_{\\mathbb{R}^{n}} \\mathcal{N} (\\bm{x}; \\bm{\\mu}, \\bm{\\Sigma}) d\\bm{x} = \\int_{\\mathbb{R}^{n}} \\mathcal{N} (\\bm{y}; \\bm{0}, \\bm{I}) \\left| \\det (\\frac{\\partial\\bm{x}}{\\partial\\bm{y}}) \\right| d\\bm{y} $$\n",
    "For $ \\frac{\\partial\\bm{x}}{\\partial\\bm{y}} $,\n",
    "$$ \\frac{\\partial\\bm{x}}{\\partial\\bm{y}} = \\frac{\\partial(\\bm{\\mu} + \\bm{\\Sigma}^{\\frac{1}{2}}\\bm{y})}{\\partial\\bm{y}} = \\bm{\\Sigma}^{\\frac{1}{2}} $$\n",
    "Thus\n",
    "$$ \\det(\\bm{\\Sigma}^{\\frac{1}{2}}) = (\\det(\\bm{\\Sigma}))^{\\frac{1}{2}} $$\n",
    "Therefore\n",
    "$$ \\int_{\\mathbb{R}^{n}} \\mathcal{N} (\\bm{x}; \\bm{\\mu}, \\bm{\\Sigma}) d\\bm{x} = \\int_{\\mathbb{R}^{n}} \\sqrt{\\frac{1}{(2\\pi)^{n}\\det(\\boldsymbol{\\Sigma})}}\\exp(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu})^\\intercal \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu})) d\\bm{x} $$\n",
    "$$ = \\int_{\\mathbb{R}^{n}} \\sqrt{\\frac{1}{(2\\pi)^{n}\\det(\\boldsymbol{\\Sigma})}}\\exp(-\\frac{1}{2} \\bm{y}^\\intercal \\boldsymbol{\\Sigma}^{-1} \\bm{y}) \\left| \\det (\\frac{\\partial\\bm{x}}{\\partial\\bm{y}}) \\right| d\\bm{y} $$\n",
    "$$ = \\int_{\\mathbb{R}^{n}} \\sqrt{\\frac{1}{(2\\pi)^{n}\\det(\\boldsymbol{\\Sigma})}}\\exp(-\\frac{1}{2} \\bm{y}^\\intercal \\boldsymbol{\\Sigma}^{-1} \\bm{y}) (\\det(\\bm{\\Sigma}))^{\\frac{1}{2}} d\\bm{y} $$\n",
    "(Since the determinant of the cov. matrix is always positive)\n",
    "$$ = \\int_{\\mathbb{R}^{n}} \\sqrt{\\frac{1}{(2\\pi)^{n}}}\\exp(-\\frac{1}{2} \\bm{y}^\\intercal \\boldsymbol{\\Sigma}^{-1} \\bm{y}) d\\bm{y} $$\n",
    "$$ = \\int_{\\mathbb{R}^{n}} \\mathcal{N} (\\bm{y}; \\bm{0}, \\bm{I}) d\\bm{y} = 1 $$\n",
    "(by `(p61.a.1)`)\n",
    "\n",
    "Which was what we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p68. Measure theory\n",
    "The probablity  of a continuous vector-valued $ \\boldsymbol{x} $ lying in some set $ \\mathbb{S} $ is given by the integral of $ p(\\boldsymbol{x}) $ over the set $ \\mathbb{S} $.\n",
    "\n",
    "Suppose we have two random variales, $ \\mathbf{x} $ and $ \\mathbf{y} $, such that $ \\boldsymbol{y} = g(\\boldsymbol{x}) $ where $ g $ is invertible, continuous, differentiable transformation.\n",
    "One might expect that \n",
    "$$ p_y(\\boldsymbol{y}) = p_x(g^{-1}(\\boldsymbol{y})) \\tag{p68.1} $$\n",
    "This is not the case.\n",
    "\n",
    "#### Q.\n",
    "- 여기서 위 $ p_x $ 와 $ p_y $ 가 무슨 함수인지 또 식 `(p68.1)` 자체가 무엇을 의미하는 지 이해가 가지 않습니다.\n",
    "- 또한 이 섹션 (3.12)의 다음 내용도 이해가 가지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p75. Undirected structured probabilistic models\n",
    "- Clique $ C^{(i)} $ 와 그에 해당하는 factor 함수 $ \\phi^{(i)} $, 그리고 noramlizing constant $ Z $ (\"defined to be the sum or integral over all states of the product of the $ \\phi $ functions\")을 이용하여 normalized probability distribution을\n",
    "$$ p(\\mathbf{x}) = \\frac{1}{Z} \\prod_i \\phi^{(i)}({\\mathcal{C}^{(i)}}) $$\n",
    "\n",
    "Q.\n",
    "- 각 클리크 $ \\mathcal{C}^{(i)} $ 와 그 factor 함수 $ \\phi^{(i)} $ 가 어떻게 정해지는 지 더 알아보고 싶습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p85. Second-order Taylor series approximation\n",
    "A Second-order Taylor series approximation to the function $ f(\\boldsymbol{x}) $ around the current point $ \\boldsymbol{x}^{(0)} $:\n",
    "$$ f(\\boldsymbol{x}) \\approx f(\\boldsymbol{x}^{(0)}) + (\\boldsymbol{x} - \\boldsymbol{x}^{(0)})^{\\intercal} \\boldsymbol{g} + \\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{x}^{(0)})^{\\intercal} \\boldsymbol{H} (\\boldsymbol{x} - \\boldsymbol{x}^{(0)}) $$\n",
    "where $ \\boldsymbol{g} $ is the gradient and $ \\boldsymbol{H} $ is the Hessian at $ \\boldsymbol{x}^{(0)} $.\n",
    "\n",
    "If we use a learning rate of $ \\epsilon $, then the new point $ \\boldsymbol{x} $ will be given by $ \\boldsymbol{x}^{(0)} - \\epsilon \\boldsymbol{g} $.\n",
    "$$ f(\\boldsymbol{x}^{(0)} - \\epsilon \\boldsymbol{g}) \\approx f(\\boldsymbol{x}^{(0)}) -\\epsilon \\boldsymbol{g}^{\\intercal} \\boldsymbol{g} + \\frac{1}{2} \\epsilon^{2} \\boldsymbol{g}^{\\intercal} \\boldsymbol{H} \\boldsymbol{g} $$\n",
    "\n",
    "#### Q.\n",
    "- 이후 내용 (Newton's method, Lipschitz continuous function) 등 학습\n",
    "- 그 다음 섹션 (4.4 Contrained Optimization) 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p192. Universal approximation theorem\n",
    "... states that a feedforward network with a linear output layer and at least on hidden layer with any \"squashing\" activation function (such as the logistic sigmoid activation function) can approximate any *Borel measurable function* from on finite-dimensional space to another with any desired nonzero amount of error, provided that the network is given enough hidden units.\n",
    "\n",
    "... for our purposes it suffices to say that any continuous function on a closed and bounded subset of $ \\mathbb{R}^n $ is Borel measurable and therefore maybe approximated by neural network.\n",
    "\n",
    "... Montufar et al. (2014) states that the number of linear regions carved out by a recitifier network with $ d $ inputs, depth $ l $, and $ n $ units per hidden layer is\n",
    "$$ O \\left( {n \\choose d}^{d(l-1)} n^{d} \\right) $$\n",
    "that is, exponential in depth $ l $. In the case of maxout networks with $ k $ filters per unit, the number of linear regions is\n",
    "$$ O \\left( k^{(l-1) + d} \\right) $$\n",
    "\n",
    "#### Q.\n",
    "- 이 내용에 대해 더 조사할 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p216. High-Order Derivatives\n",
    "Instead of explicitly computing the Hessian, the typical deep learning approach is to use *Krylov methods.* Krylov methods are a set of iterative techniques for performing various operations, such as approximately inverting a matrix of finding approximations to its eigenvectors or eigenvalues, without using any operation other than matrix-vector products.\n",
    "\n",
    "To use Krylov methods on the Hessian, we only need to be able to compute the product between the Hessian matrix $ \\boldsymbol{H} $ and an arvitrary vector $ \\boldsymbol{v} $. A straightforward technique (Christianson, 1992) for doing so is to compute\n",
    "$$ \\boldsymbol{H} \\boldsymbol{v} = \\nabla_{\\boldsymbol{x}} [(\\nabla_{\\boldsymbol{x}}f(x))^{\\intercal} \\boldsymbol{v}] \\tag{p126.1} $$\n",
    "\n",
    "#### Q.\n",
    "- 위 식 `(p126.1)`과 전체적인 내용에 대해 더 공부할 것."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
