{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- 각 단어를 일정한 벡터에 임베딩함.\n",
    "- \"For distributed representations\": 단어의 유사성을 기준으로 함.\n",
    "- 학습방식\n",
    "  - CBOW: Continuous bag-of-words: 주변 단어를 통해 중심단어를 에측.\n",
    "  - Skip-gram: 중심 단어를 통해 주변 단어를 예측."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling (n-gram)\n",
    "- N-grams: Divide the corpus with `n` word-window.\n",
    "  - `\"The reporters listened closely\"` in bigram (`n=2`): `(\"The\", \"reporters\")`, `(\"reporters\", \"listened\")`, `(\"listened\", \"closely\")`, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "\n",
    "- For processing sequential data.\n",
    "- Unfording: $ \\boldsymbol{s}^{(t)} = f(\\boldsymbol{s}^{(t-1)}; \\boldsymbol{\\theta}) = f(f(\\boldsymbol{s}^{(t-2)}; \\boldsymbol{\\theta}); \\boldsymbol{\\theta}) = ...$\n",
    "- At each *time step (or frame)* `t`, a *recurrent neuron* receives the inputs $ \\boldsymbol{x}_{(t)} $ as well as its own output from the precious time step, $ y_{(t-1)} $\n",
    "$$ \\boldsymbol{y}_{(t)} = \\phi(\\boldsymbol{W}_{x}^{\\intercal} \\boldsymbol{x}_{(t)} + \\boldsymbol{W}_{y}^{\\intercal} \\boldsymbol{y}_{(t-1)} + \\boldsymbol{b}) $$\n",
    "$$ \\boldsymbol{Y}_{(t)} = \\phi([\\boldsymbol{X}_{(t)} \\boldsymbol{Y}_{(t-1)}] \\boldsymbol{W} + \\boldsymbol{b}) $$\n",
    "- Teacher Forcing: use the ground truth $ \\boldsymbol{y}_{(t-1)} $ for the hidden state $ \\boldsymbol{h}_{(t)} $ in the training time, and in the test time use the model's output $ \\boldsymbol{o}^{(t-1)} $\n",
    "\n",
    "#### Sequences\n",
    "- Sequence-to-sequence network\n",
    "- Sequence-to-vector network (only the final output is used as output)\n",
    "  - Can be used a a encoder\n",
    "- Vector-to-sequence network\n",
    "  - Can be used a a decoder\n",
    "\n",
    "#### Difficulties\n",
    "- Unstable gradients: alleviated using recurrent dropout, recurrent layer normalization, etc.\n",
    "- Limited short-term memory: extended using LSTM, GRU, etc.\n",
    "\n",
    "#### Attention mechanisms\n",
    "- With alignment model (or attention layer), which checks if each encoder output  is aligned with the decoder's previous hidden state.\n",
    "- Adds Explainability\n",
    "\n",
    "#### The Transformer Architecture\n",
    "- \"Attention is All You Need\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "- An evaluation metric for language models.\n",
    "$$ P(w_1, w_2, ..., w_N)^{-\\frac{1}{N}} = \\sqrt[N]{\\frac{1}{P(w_1, w_2, ..., w_N)}} $$\n",
    "$$ = \\sqrt[N]{\\frac{1}{\\prod^{N}_{i=1} P(w_i | w_2, ..., w_{i-1})}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### p402 - 406. Optmization for Long-Term Dependencies\n",
    "#### Q.\n",
    "내용 이해하기. (전체 내용이 이해가 가지 않음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p113. Bayers Error\n",
    "이론적으로 도달 가능한 최소 오차.\n",
    "#### Q.\n",
    "더 조사.\n",
    "\n",
    "### p123. Sample variance vs Unbiased sample variance\n",
    "For variance parameter $ \\sigma^2 $ of a Gaussian distribution, the sample variance is\n",
    "$$ \\hat{\\sigma}_m^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x^{(i)} - \\hat{\\mu}_m)^2 $$\n",
    "where $ \\hat{\\mu}_m $ is the sample mean.\n",
    "$$ \\mathrm{bias}(\\hat{\\sigma}_m^2) = \\mathbb{E}[\\hat{\\sigma}_m^2] - \\sigma^2 $$\n",
    "For the term $ \\mathbb{E}[\\hat{\\sigma}_m^2] $,\n",
    "$$ \\mathbb{E}[\\hat{\\sigma}_m^2] = \\mathbb{E} \\left[ \\frac{1}{m} \\sum_{i=1}^{m} (x^{(i)} - \\hat{\\mu}_m)^2 \\right] $$\n",
    "$$ = \\frac{m-1}{m} \\sigma^2 \\tag{p123.1} $$\n",
    "#### Q.\n",
    "어떻게 `(p123.1)`이 도출되었는지 확인.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p126. MSE: Trading off bias and variance\n",
    "$$ \\mathrm{MSE} = \\mathbb{E}[(\\hat{\\theta}_m - \\theta)^2] $$\n",
    "$$ = \\mathrm{Bias}(\\hat{\\theta}_m)^2 + \\mathrm{Var}(\\hat{\\theta}_m) \\tag{p126.1} $$\n",
    "#### Q.\n",
    "식 `(p126.1)`이 어떻게 도출되었는지 조사."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
