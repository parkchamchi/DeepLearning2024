{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL3 (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- 각 단어를 일정한 벡터에 임베딩함.\n",
    "- \"Implements a word embedding model for distributed representations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling (n-gram)\n",
    "- N-grams: Divide the corpus with `n` word-window.\n",
    "  - `\"The reporters listened closely\"` in bigram (`n=2`): `(\"The\", \"reporters\")`, `(\"reporters\", \"listened\")`, `(\"listened\", \"closely\")`, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "\n",
    "- For processing sequential data.\n",
    "- Unfording: $ \\bm{s}^{(t)} = f(\\bm{s}^{(t-1)}; \\bm{\\theta}) = f(f(\\bm{s}^{(t-2)}; \\bm{\\theta}); \\bm{\\theta}) = ...$\n",
    "- At each *time step (or frame)* `t`, a *recurrent neuron* receives the inputs $ \\bm{x}_{(t)} $ as well as its own output from the precious time step, $ y_{(t-1)} $\n",
    "$$ \\bm{y}_{(t)} = \\phi(\\bm{W}_{x}^{\\intercal} \\bm{x}_{(t)} + \\bm{W}_{y}^{\\intercal} \\bm{y}_{(t-1)} + \\bm{b}) $$\n",
    "$$ \\bm{Y}_{(t)} = \\phi([\\bm{X}_{(t)} \\bm{Y}_{(t-1)}] \\bm{W} + \\bm{b}) $$\n",
    "- Teacher Forcing: use the ground truth $ \\bm{y}_{(t-1)} $ for the hidden state $ \\bm{h}_{(t)} $ in the training time, and in the test time use the model's output $ \\bm{o}^{(t-1)} $\n",
    "\n",
    "#### Sequences\n",
    "- Sequence-to-sequence network\n",
    "- Sequence-to-vector network (only the final output is used as output)\n",
    "  - Can be used a a encoder\n",
    "- Vector-to-sequence network\n",
    "  - Can be used a a decoder\n",
    "\n",
    "#### Difficulties\n",
    "- Unstable gradients: alleviated using recurrent dropout, recurrent layer normalization, etc.\n",
    "- Limited short-term memory: extended using LSTM, GRU, etc.\n",
    "\n",
    "#### Attention mechanisms\n",
    "- With alignment model (or attention layer), which checks if each encoder output  is aligned with the decoder's previous hidden state.\n",
    "- Adds Explainability\n",
    "\n",
    "#### The Transformer Architecture\n",
    "- \"Attention is All You Need\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "- An evaluation metric for language models.\n",
    "$$ P(w_1, w_2, ..., w_N)^{-\\frac{1}{N}} = \\sqrt[N]{\\frac{1}{P(w_1, w_2, ..., w_N)}} $$\n",
    "$$ = \\sqrt[N]{\\frac{1}{\\prod^{N}_{i=1} P(w_i | w_2, ..., w_{i-1})}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
